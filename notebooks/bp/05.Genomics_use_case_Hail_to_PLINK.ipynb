{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to filter genomic data using Hail and save to PLINK files\n",
    "\n",
    "**How to filter genomic data using Hail and save to PLINK files**\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "A common practice in Hail is to filter a user's custom list of SNPs and samples, and then save the filtered data to other formats, such as PLINK files, for downstream analysis. This notebook presents an optimized filtering workflow that follows this sequence: filtering samples, then chromosomal intervals, and finally locus and alleles. Through our testing, this workflow has significantly reduced runtime compared to a workflow without the second step of filtering chromosomal intervals.\n",
    "\n",
    "Given a list of SNPs and sample IDs, this notebook demonstrates how to extract data from acaf_threshold_v7.1/splitMT and save the result to PLINK files.\n",
    "\n",
    "This notebook is designed to run on a VM with 8 CPUs and 30 GB of memory, utilizing 2 workers and 2 preemptible workers. The estimated cost for running this notebook is approximately $1.25 per hour.\n",
    "\n",
    "Additionally, this notebook includes examples demonstrating how to create a small dataset for testing runtime and cost estimation purposes (please refer to the last section \"Tips\").\n",
    "\n",
    "**Prerequisite: We expect readers to have already gone through our tutorial genomic workspace (click to open in a new tab) https://workbench.researchallofus.org/workspaces/aou-rw-b7598f6e/duplicateofhowtoworkwithallofusgenomicdatahailplinkv7prep/data or have basic knowledge of Hail and relevant genomic file types such as Hail MatrixTable and PLINK files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup \n",
    "\n",
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up python environment\n",
    "from datetime import datetime\n",
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import hail as hl\n",
    "hl.init(default_reference = \"GRCh38\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_path = \"gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use exome split MT\n",
    "mt_wgs_path = os.getenv(\"WGS_EXOME_SPLIT_HAIL_PATH\")\n",
    "mt_wgs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read genomic data and filter out flagged samples\n",
    "\n",
    "**Read genomic data and filter out flagged samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "mt = hl.read_matrix_table(mt_wgs_path)\n",
    "\n",
    "# Prepare MT from relatedness\n",
    "flagged_samples = f'{auxiliary_path}/relatedness/relatedness_flagged_samples.tsv'\n",
    "sample_to_remove = hl.import_table(flagged_samples, key=\"sample_id\")\n",
    "\n",
    "mt = mt.anti_join_cols(sample_to_remove)\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assuming we have a list of samples (5k) and SNPs (10k) that are already saved to the bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {bucket}/data/genomics/genomic_pid_s5000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {bucket}/data/genomics/snps_hg38_s10k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter custom samples first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter custom samples first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = hl.import_table( f'{bucket}/data/genomics/genomic_pid_s5000.csv', delimiter = \",\" )\n",
    "pid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomly choose a small sample size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose an approximate fraction of samples\n",
    "pid=pid.sample(0.01)\n",
    "pid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename key and rekey file\n",
    "pid = pid.key_by( 'person_id' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter mt to desired participants\n",
    "mt = mt.filter_cols( hl.is_defined( pid[ mt.col_key ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then filter chromosome intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then filter chromosome intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start3 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in SNP file from the bucket\n",
    "file=f'{bucket}/data/genomics/snps_hg38_s10k.txt'\n",
    "snps = hl.import_table( file, delimiter = '\\t', key = 'SNP' )\n",
    "snps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomly choose a small size of SNPs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps=snps.sample(0.01)\n",
    "snps.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split column SNP to contig and position**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'SNP' column into 'contig' and 'position'\n",
    "snps = snps.annotate(contig=snps.SNP.split(\":\")[0],\n",
    "                             position=hl.int(snps.SNP.split(\":\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interval column\n",
    "snps = snps.transmute(position = hl.int32(snps.position))\n",
    "snps = snps.annotate(interval = hl.locus_interval(\n",
    "    snps.contig, snps.position, snps.position + 1)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then filter intervals first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be alert that you may encounter memory issue if the number of variants is >50k\n",
    "\n",
    "mt = hl.filter_intervals(mt, snps.interval.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out this step if needed to save runtime\n",
    "mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "end-start3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lastly, filter locus+alleles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lastly, filter locus+alleles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create locus and allele and rekey\n",
    "snps_loc = snps.annotate( locus_allele = hl.parse_variant( snps.SNP, reference_genome = 'GRCh38' ) )\n",
    "snps_loc = snps_loc.annotate( locus = snps_loc.locus_allele.locus,\n",
    "                             alleles = snps_loc.locus_allele.alleles )\n",
    "snps_loc = snps_loc.key_by( snps_loc.locus, snps_loc.alleles )\n",
    "snps_loc = snps_loc.drop( 'SNP', 'locus_allele' )\n",
    "snps_loc.show( 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter matrixtable(MT) based on matched exact locus+alleles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt2 = mt.semi_join_rows(snps_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out this step if you need to save runtime\n",
    "mt2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save to PLINK files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save fitered MT to PLINK files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start5 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{bucket}/data/test/plink_small2'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.export_plink(mt2, file_path, ind_id = mt2.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "end-start5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out these steps if you want to save to another matrixtable\n",
    "# remember matrixtable can't be saved to your local VM. It has to be in the bucket.\n",
    "\n",
    "# file_path = f'{bucket}/data/test/small2.mt'\n",
    "# mt2.write(filt_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips:\n",
    "\n",
    "1. Use mt.sample() to choose a small dataset.\n",
    "\n",
    "\n",
    "2. Feel free to test runtime with or without the 2nd step (filtering chromosomal intervals)\n",
    "\n",
    "\n",
    "3. Don't use any extra Hail commands such as mt.count() if you want to further save runtime.\n",
    "\n",
    "\n",
    "4. If you're dealing with a large dataset containing 1 million SNPs in 100,000 samples, it's advisable to start with smaller datasets for testing purposes before processing the entire dataset. These smaller datasets can include approximately 50 SNPs by 50 samples, 100 by 100, 1000 by 1000, 5000 by 5000, and up to 10,000 by 10,000. For example, when using a VM with 8 CPUs, 30 GB of RAM and utilizing 2/2 workers, processing a 5000 by 5000 dataset in this notebook took about 6 minutes. Extrapolating to a dataset with 1 million SNPs, it would take roughly 25 hours on the same VM setup. However, by scaling up to 50/50 workers, the cost increases to $20 per hour, but the runtime should decrease proportionally, potentially completing the task within 1-2 hours.\n",
    "\n",
    "\n",
    "5. Use a background notebook for jobs that need more than 30 mins of runtime (click to open in a new tab) \n",
    "\n",
    "https://workbench.researchallofus.org/workspaces/aou-rw-d56fb435/bestpracticesforaoudatascience/analysis/preview/00.How%20to%20Run%20Notebooks%20in%20the%20Background.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
